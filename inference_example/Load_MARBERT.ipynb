{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import io\n",
    "from time import time\n",
    "import argparse\n",
    "import GPUtil\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from transformers import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the prediction samples from tsv file. \n",
    "\n",
    "We provide a sample file to show the format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./narrow_country_b/inference_sample.tsv\", delimiter='\\t',header=0) \n",
    "## please change the file path to your corresponding path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Found GPU')\n",
    "# If GPU is availble, the model will train on GPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(file_path, tokenizer, max_len = 32):\n",
    "    '''\n",
    "    file_path: the path to input file. \n",
    "                The input must be a tsv file that includes only one column that is tweet text content. \n",
    "                The first row must be header of column.\n",
    "\n",
    "    lab2ind: dictionary of label classes\n",
    "    tokenizer: BERT tokenizer\n",
    "    max_len: maximal length of input sequence\n",
    "    '''\n",
    "\n",
    "    # if we are in predict mode, we will load one column (i.e., text).\n",
    "    df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content'])\n",
    "    print(\"Data size \", df.shape)\n",
    "\n",
    "        \n",
    "    # Create tweet lists\n",
    "    contents = df.content.values\n",
    "\n",
    "    # We need to add a special token at the beginning for BERT to work properly.\n",
    "    content = [\"[CLS] \" + text for text in contents]\n",
    "\n",
    "    # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
    "    tokenized_texts = [tokenizer.tokenize(text) for text in content]\n",
    "\n",
    "    # if the sequence is longer the maximal length, we truncate it to the pre-defined maximal length\n",
    "    tokenized_texts = [ text[:max_len+1] for text in tokenized_texts]\n",
    "\n",
    "    # We also need to add a special token at the end.\n",
    "    tokenized_texts = [ text+['[SEP]'] for text in tokenized_texts]\n",
    "    print (\"Tokenize the first sentence:\\n\",tokenized_texts[0])\n",
    "    \n",
    "    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    print (\"Index numbers of the first sentence:\\n\",input_ids[0])\n",
    "\n",
    "    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
    "    pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=max_len+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
    "    print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
    "\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # Create a mask of 1s for each token followed by 0s for pad tokens\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # Convert all of our data into torch tensors, the required datatype for our model\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, iterator, ind2label):\n",
    "    \n",
    "    model.eval()\n",
    "    # output lists\n",
    "    output_prob=[]\n",
    "    output_label = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            input_ids, input_mask = batch\n",
    "            \n",
    "            outputs = model(input_ids, input_mask)\n",
    "            logits = outputs[0]\n",
    "\n",
    "            prob = F.softmax(logits, dim=1)\n",
    "            # delete used variables to free GPU memory\n",
    "            del batch, input_ids, input_mask\n",
    "\n",
    "            # identify the predicted class and the probability\n",
    "            probabilities, predicted = torch.max(prob.cpu().data, 1)\n",
    "\n",
    "            # put the probability of the predicted label to a list \n",
    "            output_prob.extend(probabilities.tolist())\n",
    "\n",
    "            # put all predicted labels to a list\n",
    "            output_label.extend([ind2label[pred] for pred in predicted.tolist()])\n",
    "            \n",
    "    return output_label, output_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the label-to-index dictionary and create index-to-label dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_file = open(os.path.join(\"./narrow_country_b/\", \"label2ind.json\")) \n",
    "## please change the file path to your corresponding path\n",
    "lab2ind= json.load(tmp_file)\n",
    "tmp_file.close()\n",
    "# create index-to-label dictionary\n",
    "ind2label= {v:k for k,v in lab2ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of labels: \", len(ind2label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the maximum sequence length for the input.\n",
    "\n",
    "We use a maximum sequence length of 50 words in fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the fine-tuned MARBERT checkpoint and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# give the model path where includes: config.json, vocab.txt, and pytorch_model.bin\n",
    "model_path = \"./narrow_country_b/\"\n",
    "## please change the file path to your corresponding path\n",
    "\n",
    "# load tokenizer from pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=False)\n",
    "\n",
    "# Load fine-tuned MARBERT model\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lab2ind))\n",
    "\n",
    "# send model to device CPU or GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use defined funtion to extract data\n",
    "inf_inputs, inf_masks = data_prepare(\"./narrow_country_b/inference_sample.tsv\", tokenizer, max_seq_length)\n",
    "## please change the file path to your corresponding path\n",
    "\n",
    "# Select a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# create a dataloader\n",
    "inf_data = TensorDataset(inf_inputs, inf_masks)\n",
    "inf_dataloader = DataLoader(inf_data, batch_size = batch_size, shuffle= False)\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_label, output_prob = inference(model, inf_dataloader, ind2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_label)\n",
    "print(output_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(output_label)):\n",
    "    print(\"Sample {} is predicted as {} with {} of confidence.\".format(i+1, output_label[i], output_prob[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
